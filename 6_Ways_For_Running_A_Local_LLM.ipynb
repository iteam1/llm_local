{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ca78cc-1abb-48ba-8730-a1d9ab7fe861",
   "metadata": {},
   "source": [
    "# 1. Hugging Face and Transformers\n",
    "\n",
    "Transformers pros:\n",
    "\n",
    "    Automatic model downloads\n",
    "    Code snippets available\n",
    "    Ideal for experimentation and learning\n",
    "\n",
    "Transformers cons:\n",
    "\n",
    "    Requires solid understanding of ML and NLP\n",
    "    Coding and configuration skills are necessary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1976f20f-1346-4c85-a849-8982cfff4aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be63723ef8b47ee8e81223d0127a2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User: Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: Hello! :D\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User: How are you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm good, how are you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User: I am good\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: That's good\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User: OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm good\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User: Do you know my name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I do\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\", padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "# source: https://huggingface.co/microsoft/DialoGPT-medium\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last output tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4b8c1-c6dc-41b8-ae87-05d0721547f3",
   "metadata": {},
   "source": [
    "# 2. LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b7c0443-cf3b-45b1-8079-fcd36e8a0862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device has 2 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is electroencephalography?\n",
      "\n",
      "Answer: Let's think step by step.I'm not a neuroscientist, but I'm pretty sure it's a branch of neuroscience.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"microsoft/DialoGPT-medium\", task=\"text-generation\", pipeline_kwargs={\"max_new_tokens\": 200, \"pad_token_id\": 50256},\n",
    ")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | hf\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d0fe13-f1b7-4e90-9508-faa85cb5b3af",
   "metadata": {},
   "source": [
    "# 3. Llama.cpp\n",
    "\n",
    "[Llama.cpp](https://github.com/ggerganov/llama.cpp) is a C and C++ based inference engine for LLMs, optimized for Apple silicon and running Meta’s Llama2 models.\n",
    "\n",
    "Once we clone the repository and build the project, we can run a model with: `$ ./main -m /path/to/model-file.gguf -p \"Hi there!\"`\n",
    "\n",
    "Llama.cpp Pros:\n",
    "\n",
    "    Higher performance than Python-based solutions\n",
    "    Supports large models like Llama 7B on modest hardware\n",
    "    Provides bindings to build AI applications with other languages while running the inference via Llama.cpp.\n",
    "\n",
    "Llama.cpp Cons:\n",
    "\n",
    "    Limited model support\n",
    "    Requires tool building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26f5e30-3760-40cc-8ff9-27e87e48b704",
   "metadata": {},
   "source": [
    "# 4. Llamafile\n",
    "\n",
    "[Llamafile](https://github.com/Mozilla-Ocho/llamafile), developed by Mozilla, offers a user-friendly alternative for running LLMs. Llamafile is known for its portability and the ability to create single-file executables.\n",
    "\n",
    "Once we download llamafile and any GGUF-formatted model, we can start a local browser session with: `$ ./llamafile -m /path/to/model.gguf`\n",
    "\n",
    "Llamafile pros:\n",
    "\n",
    "    Same speed benefits as Llama.cpp\n",
    "    You can build a single executable file with the model embedded\n",
    "\n",
    "Llamafile cons:\n",
    "\n",
    "    The project is still in the early stages\n",
    "    Not all models are supported, only the ones Llama.cpp supports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b0918-b041-4cd6-ace1-72456eaee6d5",
   "metadata": {},
   "source": [
    "# 5. Ollama\n",
    "\n",
    "[Ollama](https://ollama.com/) is a more user-friendly alternative to Llama.cpp and Llamafile. You download an executable that installs a service on your machine. Once installed, you open a terminal and run: `$ ollama run llama2`\n",
    "\n",
    "Ollama pros:\n",
    "\n",
    "    Easy to install and use.\n",
    "    Can run llama and vicuña models.\n",
    "    It is really fast.\n",
    "\n",
    "Ollama cons:\n",
    "\n",
    "    Provides limited model library.\n",
    "    Manages models by itself, you cannot reuse your own models.\n",
    "    Not tunable options to run the LLM.\n",
    "    No Windows version (yet).\n",
    "\n",
    "[model library](https://ollama.com/library)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f1cf4-be45-4f34-a1b7-140e90382c40",
   "metadata": {},
   "source": [
    "# 6. GPT4ALL\n",
    "\n",
    "GPT4ALL is an easy-to-use desktop application with an intuitive GUI. It supports local model running and offers connectivity to OpenAI with an API key. It stands out for its ability to process local documents for context, ensuring privacy.\n",
    "\n",
    "GPT4ALL Pros:\n",
    "\n",
    "    Polished alternative with a friendly UI\n",
    "    Supports a range of curated models\n",
    "\n",
    "GPT4ALL Cons:\n",
    "\n",
    "    Limited model selection\n",
    "    Some models have commercial usage restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7d75f3-1863-4ecc-9234-c3a44aca9ed8",
   "metadata": {},
   "source": [
    "# references\n",
    "\n",
    "[6 Ways For Running A Local LLM (how to use HuggingFace)](https://semaphoreci.com/blog/local-llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
