# template

 run llm model on local system by llamafile 

# guide

1. Download [llava-v1.5-7b-q4.llamafile](https://huggingface.co/Mozilla/llava-v1.5-7b-llamafile/resolve/main/llava-v1.5-7b-q4.llamafile?download=true) (4.29 GB)

2. Grant permission for your computer to execute this new file (If you're on Windows, rename the file by adding ".exe" on the end):

        chmod +x llava-v1.5-7b-q4.llamafile

3. Run the llamafile (On your browser http://localhost:8080/) `./llava-v1.5-7b-q4.llamafile`

# shout down for

[llamafile](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file)

[llama.cpp](https://github.com/ggerganov/llama.cpp)

[llama-cpp-python](https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file)